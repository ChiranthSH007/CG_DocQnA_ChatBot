{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import pandas as pd\n",
    "import time \n",
    "import io\n",
    "from tqdm import tqdm\n",
    "# from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     max_length=128,\n",
    "#     temperature=0.5,\n",
    "#     huggingfacehub_api_token=\"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\",\n",
    "# )\n",
    "# llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPDFBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file_path=\"\"\n",
    "        self.user_input=\"\"\n",
    "        self.sec_id=\"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\"\n",
    "        self.repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    def build_vectordb(self,chunk_size,overlap):\n",
    "        loader = PyPDFLoader(\"C:/Users/chirsh/OneDrive - Capgemini/Desktop/Python/DocumentQnAChatBot/ChatGPTEBook.pdf\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=overlap)\n",
    "        self.index = VectorstoreIndexCreator(embedding=HuggingFaceEmbeddings(),text_splitter=text_splitter).from_loaders([loader])\n",
    "\n",
    "    def load_model(self,n_threads,max_tokens,repeat_penalty,n_batch,top_k,temp):\n",
    "        callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "        self.llm = HuggingFaceEndpoint(\n",
    "            repo_id=self.repo_id,\n",
    "            max_length=128,\n",
    "            temperature=0.5,\n",
    "            huggingfacehub_api_token=self.sec_id,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "    def retrieval(self,user_input,top_k=1,context_verbosity = False,rag_off=False):\n",
    "        self.user_input = user_input\n",
    "        self.context_verbosity = context_verbosity\n",
    "        result = self.index.vectorstore.similarity_search(self.user_input,k=top_k)\n",
    "        context = \"\\n\".join([document.page_content for document in result])\n",
    "\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Retrieving information related to your question...\")\n",
    "            print(f\"Found this content which is most similar to your question:{context}\")\n",
    "\n",
    "        if rag_off:\n",
    "            template = \"\"\"Question: {question}\n",
    "            Answer: This is the response:\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "        else:\n",
    "            template=\"\"\"Dont't just repeat  the following context, use it in conbination with your knowledge to improve your answer to the question: {context}\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template,input_variables=[\"context\",\"question\"]).partial(context=context)\n",
    "\n",
    "    def inference(self):\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Your Query: {self.prompt}\")\n",
    "        \n",
    "        llm_chain = self.prompt | self.llm\n",
    "        print(f\"Processing the information...\\n\")\n",
    "        response =llm_chain.invoke({\"question\": self.user_input})\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0ecfa708a542c7932634da47935642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Checkbox(value=False, description='RAG OFF?', indent=False, tooltip='Turns off Râ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa66391e3e947fdbdb9e93617d5d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "bot = RAGPDFBot()\n",
    "\n",
    "# Initialize previous value variables\n",
    "model_loaded=False\n",
    "\n",
    "# Create an output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_inputs(b):\n",
    "    \n",
    "    global model_loaded\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # Suppress output\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "\n",
    "            # Function to process inputs\n",
    "            # Gather values from the widgets\n",
    "            query = query_text.value\n",
    "            top_k = 2\n",
    "            chunk_size = 500\n",
    "            overlap = 50\n",
    "            threads = 64\n",
    "            max_tokens = 50\n",
    "            rag_off = rag_off_checkbox.value\n",
    "            temp = 0.7\n",
    "\n",
    "            if model_loaded==False:\n",
    "                print(\"loading model due incorporate new parameters\")\n",
    "                bot.load_model(n_threads=threads, max_tokens=max_tokens, repeat_penalty=1.50, n_batch=threads, top_k=top_k, temp=temp)\n",
    "                model_loaded=True\n",
    "                #build the vector database\n",
    "                print(\"rebuilding vector DB due to changing dataset, overlap, or chunk\")\n",
    "                bot.build_vectordb(chunk_size = chunk_size, overlap = overlap)\n",
    "\n",
    "            bot.retrieval(user_input = query, rag_off = rag_off)\n",
    "            response = bot.inference()\n",
    "    \n",
    "            styled_response = f\"\"\"\n",
    "            <div style=\"\n",
    "                background-color: lightblue;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                color: black;\n",
    "                max-width: 600px;\n",
    "                word-wrap: break-word;\n",
    "                margin: 10px;\n",
    "                font-size: 14px;\">\n",
    "                {response}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            display(HTML(styled_response))\n",
    "\n",
    "def create_chat_interface():\n",
    "    global query_text, rag_off_checkbox\n",
    "    \n",
    "    # User query text input\n",
    "    query_layout = widgets.Layout(width='400px', height='200px')  # Adjust the width as needed\n",
    "    query_text = widgets.Text(\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False, \n",
    "        layout=query_layout\n",
    "    )\n",
    "\n",
    "    # RAG OFF TOGGLE\n",
    "    rag_off_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='RAG OFF?',\n",
    "    disabled=False,\n",
    "    indent=False,  # Set to True if you want the checkbox to be indented\n",
    "    tooltip='Turns off RAG and Performs Inference with Raw Model and Prompt Only'\n",
    "    )\n",
    "\n",
    "    # Group the widgets except the query text into a VBox\n",
    "    left_column = widgets.VBox([rag_off_checkbox])\n",
    "\n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(process_inputs)\n",
    "\n",
    "    right_column = widgets.VBox([query_text, submit_button])\n",
    "\n",
    "    # Use HBox to position the VBox and query text side by side\n",
    "    interface_layout = widgets.HBox([left_column, right_column])\n",
    "\n",
    "\n",
    "    # Display the layout\n",
    "    display(interface_layout, output)\n",
    "\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
